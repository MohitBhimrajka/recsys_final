{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split # Can use this for a quick validation set\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "# Add project root to sys.path\n",
    "project_root = Path.cwd().parent # Should be RECSYS_FINAL\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src import config\n",
    "from src.data.dataset import CFDataset, create_mappings_and_unique_ids # Import dataset class and helper\n",
    "from src.models.ncf import NCF # Import the NCF model\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final aggregated interactions data\n",
    "interactions_path = config.PROCESSED_DATA_DIR / \"interactions_final.parquet\"\n",
    "try:\n",
    "    interactions_df = pd.read_parquet(interactions_path)\n",
    "    print(f\"Loaded interactions data shape: {interactions_df.shape}\")\n",
    "    print(interactions_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {interactions_path} not found.\")\n",
    "    print(\"Please ensure the preprocessing pipeline (run_preprocessing.py) has run successfully.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "USER_COL = 'id_student'\n",
    "ITEM_COL = 'presentation_id'\n",
    "\n",
    "# Create mappings from original IDs to contiguous indices\n",
    "user_id_map, item_id_map, unique_users, unique_items = create_mappings_and_unique_ids(\n",
    "    interactions_df, USER_COL, ITEM_COL\n",
    ")\n",
    "n_users = len(unique_users)\n",
    "n_items = len(unique_items)\n",
    "\n",
    "print(f\"Number of unique users: {n_users}\")\n",
    "print(f\"Number of unique items: {n_items}\")\n",
    "\n",
    "# Split interactions data (optional, for quick validation during training)\n",
    "# Using a simple random split here just for dev purposes.\n",
    "# The final evaluation will use the proper time-based split test_df.\n",
    "train_interactions, val_interactions = train_test_split(\n",
    "    interactions_df, test_size=0.1, random_state=config.RANDOM_SEED\n",
    ")\n",
    "print(f\"Train interactions shape: {train_interactions.shape}\")\n",
    "print(f\"Validation interactions shape: {val_interactions.shape}\")\n",
    "\n",
    "# Create Datasets (using the full mappings created from the whole interactions_df)\n",
    "# Training dataset WITH negative sampling\n",
    "train_dataset = CFDataset(\n",
    "    interactions_df=train_interactions,\n",
    "    all_item_ids=unique_items.tolist(), # Pass all unique items\n",
    "    user_id_map=user_id_map,\n",
    "    item_id_map=item_id_map,\n",
    "    user_col=USER_COL,\n",
    "    item_col=ITEM_COL,\n",
    "    num_negatives=4 # Example: 4 negative samples per positive\n",
    ")\n",
    "\n",
    "# Validation dataset WITHOUT negative sampling (only positive interactions)\n",
    "# We will predict scores for these and compare against a threshold or use ranking metrics\n",
    "val_dataset = CFDataset(\n",
    "    interactions_df=val_interactions,\n",
    "    all_item_ids=unique_items.tolist(),\n",
    "    user_id_map=user_id_map,\n",
    "    item_id_map=item_id_map,\n",
    "    user_col=USER_COL,\n",
    "    item_col=ITEM_COL,\n",
    "    num_negatives=0 # No negative sampling for validation of positives\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 1024 # Adjust based on memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=4, pin_memory=True) # Usually larger batch size for validation\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch size: {BATCH_SIZE} (train), {BATCH_SIZE*2} (val)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === New Cell: Instantiate and Train NCFRecommender ===\n",
    "from src.models.ncf import NCFRecommender # Import the wrapper\n",
    "\n",
    "# Define hyperparameters for the wrapper\n",
    "MF_DIM_WRAP = 32\n",
    "MLP_EMBEDDING_DIM_WRAP = 32\n",
    "MLP_LAYERS_WRAP = [64, 32, 16, 8]\n",
    "DROPOUT_WRAP = 0.2\n",
    "LEARNING_RATE_WRAP = 0.001\n",
    "EPOCHS_WRAP = 2 # Train for only 2 epochs as before\n",
    "WEIGHT_DECAY_WRAP = 1e-5\n",
    "BATCH_SIZE_WRAP = 1024 # Match DataLoader batch size used before\n",
    "NUM_NEGATIVES_WRAP = 4 # Match negative samples used before\n",
    "\n",
    "print(\"\\n--- Initializing NCFRecommender ---\")\n",
    "ncf_recommender = NCFRecommender(\n",
    "    user_col=USER_COL, # Defined earlier in notebook\n",
    "    item_col=ITEM_COL, # Defined earlier in notebook\n",
    "    mf_dim=MF_DIM_WRAP,\n",
    "    mlp_layers=MLP_LAYERS_WRAP,\n",
    "    mlp_embedding_dim=MLP_EMBEDDING_DIM_WRAP,\n",
    "    dropout=DROPOUT_WRAP,\n",
    "    learning_rate=LEARNING_RATE_WRAP,\n",
    "    epochs=EPOCHS_WRAP,\n",
    "    batch_size=BATCH_SIZE_WRAP,\n",
    "    num_negatives=NUM_NEGATIVES_WRAP,\n",
    "    weight_decay=WEIGHT_DECAY_WRAP,\n",
    "    device='auto' # Or specify 'cuda'/'cpu'\n",
    ")\n",
    "\n",
    "# Train the model using the 'fit' method of the wrapper\n",
    "# Pass the full interactions_df used to create mappings/dataset originally\n",
    "print(\"\\n--- Training NCFRecommender ---\")\n",
    "# Make sure interactions_df, USER_COL, ITEM_COL are defined from earlier cells\n",
    "# Ensure 'interactions_df' is the full dataset intended for training this instance\n",
    "# For dev, you might use 'train_interactions' if you only want to fit on the dev split\n",
    "# ncf_recommender.fit(train_interactions) # Option 1: Fit on dev split\n",
    "ncf_recommender.fit(interactions_df)      # Option 2: Fit on full data\n",
    "\n",
    "print(\"\\n--- NCFRecommender Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Evaluate NCF Model (Corrected WITH Wrapper)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Ensure project root is in sys.path ---\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "# -----------------------------------------\n",
    "\n",
    "# --- Import necessary functions/classes ---\n",
    "from src import config\n",
    "from src.data import preprocess # For time_based_split\n",
    "from src.evaluation.evaluator import RecEvaluator\n",
    "#------------------------------------------\n",
    "\n",
    "# --- Ensure necessary variables are defined (Check before proceeding) ---\n",
    "# --- MODIFIED CHECK: Check for the wrapper instance ---\n",
    "if 'ncf_recommender' not in locals():\n",
    "    raise NameError(\"NCFRecommender instance 'ncf_recommender' not defined. Run the training cell first.\")\n",
    "# -------------------------------------------------------\n",
    "if 'user_id_map' not in locals(): raise NameError(\"'user_id_map' not defined. Run cell [3] first.\") # Keep these checks\n",
    "if 'item_id_map' not in locals(): raise NameError(\"'item_id_map' not defined. Run cell [3] first.\")\n",
    "#-----------------------------------------------\n",
    "\n",
    "# --- Load or Recreate the CORRECT Time-Based Train/Test Split ---\n",
    "# (This section remains the same)\n",
    "print(\"Recreating time-based split for evaluation...\")\n",
    "interactions_path_eval = config.PROCESSED_DATA_DIR / \"interactions_final.parquet\"\n",
    "# Use interactions_df if already loaded, otherwise load it\n",
    "if 'interactions_df' not in locals() or not isinstance(interactions_df, pd.DataFrame):\n",
    "    if not interactions_path_eval.exists():\n",
    "         raise FileNotFoundError(f\"Cannot find {interactions_path_eval}. Run preprocessing first.\")\n",
    "    interactions_df_eval = pd.read_parquet(interactions_path_eval) # Use a different name to avoid confusion if needed\n",
    "else:\n",
    "    interactions_df_eval = interactions_df # Use the one already loaded\n",
    "\n",
    "TIME_THRESHOLD = config.TIME_SPLIT_THRESHOLD # <<< USE CONFIG VALUE\n",
    "train_df_eval, test_df_eval = preprocess.time_based_split(\n",
    "    interactions_df=interactions_df_eval, # Use the potentially reloaded DF\n",
    "    user_col='id_student',\n",
    "    item_col='presentation_id',\n",
    "    time_col='last_interaction_date',\n",
    "    time_unit_threshold=TIME_THRESHOLD\n",
    ")\n",
    "print(f\"Time-based split recreated. Train: {train_df_eval.shape}, Test: {test_df_eval.shape}\")\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# --- Load Item Features ---\n",
    "# (This section remains the same)\n",
    "items_df_path = config.PROCESSED_DATA_DIR / \"items_final.parquet\"\n",
    "# Use items_df if already loaded and correctly indexed, otherwise load it\n",
    "if 'items_df' not in locals() or not isinstance(items_df, pd.DataFrame) or items_df.index.name != 'presentation_id':\n",
    "     print(\"Loading items_df...\")\n",
    "     items_df_eval = pd.read_parquet(items_df_path) # Use a different name\n",
    "     if 'presentation_id' in items_df_eval.columns:\n",
    "        items_df_eval = items_df_eval.set_index('presentation_id')\n",
    "     elif items_df_eval.index.name == 'presentation_id':\n",
    "         pass # Already indexed correctly\n",
    "     else:\n",
    "        raise ValueError(\"Items DataFrame must have 'presentation_id' column or index.\")\n",
    "else:\n",
    "    items_df_eval = items_df # Use the one already loaded\n",
    "\n",
    "print(\"Items DataFrame ready for evaluator.\")\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# --- NO WRAPPER NEEDED HERE - Model is already wrapped ---\n",
    "# --- (Delete the old NCFEvaluatorWrapper class definition if it's still here) ---\n",
    "# --- (Delete the old ncf_eval_wrapper = ... line if it's still here) ---\n",
    "# -----------------------------------------------------------------\n",
    "print(\"Using the trained 'ncf_recommender' instance directly.\")\n",
    "\n",
    "# --- Initialize Evaluator and Evaluate ---\n",
    "if test_df_eval.empty:\n",
    "    print(\"\\nCannot evaluate NCF model: Test data (time-split) is empty.\")\n",
    "elif items_df_eval.index.name != 'presentation_id': # Check the correct items_df variable\n",
    "    print(\"\\nError: items_df_eval must have 'presentation_id' set as index for evaluator.\")\n",
    "else:\n",
    "    print(f\"\\nInitializing evaluator with Train: {train_df_eval.shape}, Test: {test_df_eval.shape}\")\n",
    "    ncf_evaluator = RecEvaluator(\n",
    "        train_df=train_df_eval,\n",
    "        test_df=test_df_eval,\n",
    "        item_features_df=items_df_eval, # Pass the correctly loaded/indexed items_df\n",
    "        user_col='id_student',\n",
    "        item_col='presentation_id',\n",
    "        k=config.TOP_K\n",
    "    )\n",
    "\n",
    "    # --- MODIFIED EVALUATION CALL: Use the wrapper instance ---\n",
    "    print(\"\\n--- Starting Evaluation of NCFRecommender ---\")\n",
    "    # Use the 'ncf_recommender' variable from the training cell\n",
    "    ncf_results = ncf_evaluator.evaluate_model(ncf_recommender, n_neg_samples=100)\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    print(\"\\nNCF Model Evaluation Results:\")\n",
    "    print(ncf_results)\n",
    "#-----------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
