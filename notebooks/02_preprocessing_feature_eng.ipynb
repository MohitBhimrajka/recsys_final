{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to sys.path (not src)\n",
    "project_root = Path.cwd().parent  # Should be RECSYS_FINAL\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import specific modules/functions we want to test\n",
    "from src import config\n",
    "from src.data import load_raw\n",
    "from src.data import utils\n",
    "from src.data import preprocess  # Import the main preprocessing module\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"Setup complete. Modules imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading all raw data\n",
    "try:\n",
    "    raw_data = load_raw.load_all_raw_data()\n",
    "    print(\"\\nRaw data loaded successfully into 'raw_data' dictionary.\")\n",
    "    # Display shapes\n",
    "    for name, df in raw_data.items():\n",
    "        print(f\"- {name}: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading raw data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cleaning functions (one by one)\n",
    "print(\"--- Testing Cleaning Functions ---\")\n",
    "student_info_clean = preprocess.clean_student_info(raw_data['student_info'])\n",
    "registrations_clean = preprocess.clean_registrations(raw_data['student_registration'])\n",
    "assessments_clean = preprocess.clean_assessments(raw_data['assessments'])\n",
    "student_assessment_clean = preprocess.clean_student_assessment(raw_data['student_assessment'])\n",
    "vle_clean = preprocess.clean_vle(raw_data['vle'])\n",
    "student_vle_clean = preprocess.clean_student_vle(raw_data['student_vle'])\n",
    "print(\"--- Finished Testing Cleaning Functions ---\")\n",
    "# Optional: Print heads/info if needed for debugging\n",
    "# print(\"\\nCleaned studentInfo Head:\\n\", student_info_clean.head())\n",
    "# print(\"\\nCleaned registrations Head:\\n\", registrations_clean.head())\n",
    "# print(\"\\nCleaned assessments Head:\\n\", assessments_clean.head())\n",
    "# print(\"\\nCleaned studentAssessment Head:\\n\", student_assessment_clean.head())\n",
    "# print(\"\\nCleaned vle Head:\\n\", vle_clean.head())\n",
    "# print(\"\\nCleaned studentVle Head:\\n\", student_vle_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test filtering interactions by registration dates\n",
    "print(\"\\n--- Testing Registration Filtering ---\")\n",
    "interactions_filtered = preprocess.filter_interactions_by_registration(\n",
    "    student_vle_clean, registrations_clean\n",
    ")\n",
    "print(\"\\nFiltered Interactions Head:\\n\", interactions_filtered.head())\n",
    "print(f\"\\nShape after filtering by registration: {interactions_filtered.shape}\")\n",
    "print(\"--- Finished Testing Registration Filtering ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test applying interaction count filters (BEFORE aggregation)\n",
    "print(\"\\n--- Testing Interaction Count Filtering ---\")\n",
    "# Use the default thresholds from config.py\n",
    "interactions_count_filtered = preprocess.apply_interaction_count_filters(\n",
    "    interactions_filtered # Apply to the output of the previous step\n",
    ")\n",
    "print(\"\\nInteractions after Interaction Count Filters Head:\\n\", interactions_count_filtered.head())\n",
    "print(f\"\\nShape after interaction count filters: {interactions_count_filtered.shape}\")\n",
    "print(f\"\\nUnique users remaining: {interactions_count_filtered['id_student'].nunique()}\")\n",
    "print(f\"Unique items remaining: {interactions_count_filtered['presentation_id'].nunique()}\")\n",
    "print(\"--- Finished Testing Interaction Count Filtering ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test creating aggregated interaction features (AFTER count filtering)\n",
    "print(\"\\n--- Testing Interaction Aggregation ---\")\n",
    "aggregated_interactions = preprocess.create_interaction_features(\n",
    "    interactions_count_filtered # Apply to the output of the previous step\n",
    ")\n",
    "print(\"\\nAggregated Interactions Head:\\n\", aggregated_interactions.head())\n",
    "print(f\"\\nShape of aggregated interactions: {aggregated_interactions.shape}\")\n",
    "\n",
    "# Plot distribution of implicit feedback score if aggregation is not empty\n",
    "if not aggregated_interactions.empty:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(aggregated_interactions['implicit_feedback'], bins=50, kde=True)\n",
    "    plt.title('Distribution of Implicit Feedback Score (log1p(total_clicks))')\n",
    "    plt.xlabel('Implicit Feedback Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nAggregated interactions DataFrame is empty, skipping plot.\")\n",
    "print(\"--- Finished Testing Interaction Aggregation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing User Feature Generation ---\")\n",
    "# Generate user features for the valid users found *after filtering and aggregation*\n",
    "if not aggregated_interactions.empty:\n",
    "    valid_user_ids_test = aggregated_interactions['id_student'].unique()\n",
    "    print(f\"Number of valid users for feature generation: {len(valid_user_ids_test)}\")\n",
    "    users_features_test = preprocess.generate_user_features(\n",
    "        student_info_clean, # Pass the cleaned student info\n",
    "        valid_user_ids_test # Pass the list of valid IDs\n",
    "    )\n",
    "    print(\"\\nUser Features Head:\\n\", users_features_test.head())\n",
    "    print(f\"\\nShape of user features: {users_features_test.shape}\")\n",
    "    # Verify shape matches unique user count\n",
    "    assert users_features_test.shape[0] == len(valid_user_ids_test)\n",
    "else:\n",
    "    print(\"Skipping user feature generation as aggregated_interactions is empty.\")\n",
    "    users_features_test = pd.DataFrame() # Assign empty dataframe\n",
    "print(\"--- Finished Testing User Feature Generation ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing Item Feature Generation ---\")\n",
    "# Generate item features for the valid items found *after filtering and aggregation*\n",
    "if not aggregated_interactions.empty:\n",
    "    valid_item_ids_test = aggregated_interactions['presentation_id'].unique()\n",
    "    print(f\"Number of valid items for feature generation: {len(valid_item_ids_test)}\")\n",
    "    # Need courses_df with presentation_id\n",
    "    courses_with_pres_id = utils.create_presentation_id(raw_data['courses'])\n",
    "    items_features_test = preprocess.generate_item_features(\n",
    "        courses_with_pres_id, # Pass cleaned courses\n",
    "        vle_clean, # Pass cleaned VLE info\n",
    "        valid_item_ids_test # Pass the list of valid IDs\n",
    "    )\n",
    "    print(\"\\nItem Features Head:\\n\", items_features_test.head())\n",
    "    print(f\"\\nShape of item features: {items_features_test.shape}\")\n",
    "    # Verify shape matches unique item count\n",
    "    assert items_features_test.shape[0] == len(valid_item_ids_test)\n",
    "else:\n",
    "    print(\"Skipping item feature generation as aggregated_interactions is empty.\")\n",
    "    items_features_test = pd.DataFrame() # Assign empty dataframe\n",
    "print(\"--- Finished Testing Item Feature Generation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9] - Optional: Test Full Pipeline\n",
    "# print(\"\\n--- Testing full preprocess_all_data() function ---\")\n",
    "# processed_data_test = preprocess.preprocess_all_data()\n",
    "# print(\"\\n--- Full pipeline test finished ---\")\n",
    "# print(f\"Final Users shape: {processed_data_test['users'].shape}\")\n",
    "# print(f\"Final Items shape: {processed_data_test['items'].shape}\")\n",
    "# print(f\"Final Interactions shape: {processed_data_test['interactions'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
