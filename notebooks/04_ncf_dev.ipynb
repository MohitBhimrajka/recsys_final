{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split # Can use this for a quick validation set\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "# Add project root to sys.path\n",
    "project_root = Path.cwd().parent # Should be RECSYS_FINAL\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src import config\n",
    "from src.data.dataset import CFDataset, create_mappings_and_unique_ids # Import dataset class and helper\n",
    "from src.models.ncf import NCF # Import the NCF model\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final aggregated interactions data\n",
    "interactions_path = config.PROCESSED_DATA_DIR / \"interactions_final.parquet\"\n",
    "try:\n",
    "    interactions_df = pd.read_parquet(interactions_path)\n",
    "    print(f\"Loaded interactions data shape: {interactions_df.shape}\")\n",
    "    print(interactions_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {interactions_path} not found.\")\n",
    "    print(\"Please ensure the preprocessing pipeline (run_preprocessing.py) has run successfully.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "USER_COL = 'id_student'\n",
    "ITEM_COL = 'presentation_id'\n",
    "\n",
    "# Create mappings from original IDs to contiguous indices\n",
    "user_id_map, item_id_map, unique_users, unique_items = create_mappings_and_unique_ids(\n",
    "    interactions_df, USER_COL, ITEM_COL\n",
    ")\n",
    "n_users = len(unique_users)\n",
    "n_items = len(unique_items)\n",
    "\n",
    "print(f\"Number of unique users: {n_users}\")\n",
    "print(f\"Number of unique items: {n_items}\")\n",
    "\n",
    "# Split interactions data (optional, for quick validation during training)\n",
    "# Using a simple random split here just for dev purposes.\n",
    "# The final evaluation will use the proper time-based split test_df.\n",
    "train_interactions, val_interactions = train_test_split(\n",
    "    interactions_df, test_size=0.1, random_state=config.RANDOM_SEED\n",
    ")\n",
    "print(f\"Train interactions shape: {train_interactions.shape}\")\n",
    "print(f\"Validation interactions shape: {val_interactions.shape}\")\n",
    "\n",
    "# Create Datasets (using the full mappings created from the whole interactions_df)\n",
    "# Training dataset WITH negative sampling\n",
    "train_dataset = CFDataset(\n",
    "    interactions_df=train_interactions,\n",
    "    all_item_ids=unique_items.tolist(), # Pass all unique items\n",
    "    user_id_map=user_id_map,\n",
    "    item_id_map=item_id_map,\n",
    "    user_col=USER_COL,\n",
    "    item_col=ITEM_COL,\n",
    "    num_negatives=4 # Example: 4 negative samples per positive\n",
    ")\n",
    "\n",
    "# Validation dataset WITHOUT negative sampling (only positive interactions)\n",
    "# We will predict scores for these and compare against a threshold or use ranking metrics\n",
    "val_dataset = CFDataset(\n",
    "    interactions_df=val_interactions,\n",
    "    all_item_ids=unique_items.tolist(),\n",
    "    user_id_map=user_id_map,\n",
    "    item_id_map=item_id_map,\n",
    "    user_col=USER_COL,\n",
    "    item_col=ITEM_COL,\n",
    "    num_negatives=0 # No negative sampling for validation of positives\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 1024 # Adjust based on memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=4, pin_memory=True) # Usually larger batch size for validation\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch size: {BATCH_SIZE} (train), {BATCH_SIZE*2} (val)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] (Modified for Weight Decay and 2 Epochs)\n",
    "\n",
    "# Model Hyperparameters (adjust as needed)\n",
    "MF_DIM = 32\n",
    "MLP_EMBEDDING_DIM = 32\n",
    "MLP_LAYERS = [64, 32, 16, 8] # Input layer (2*MLP_EMBEDDING_DIM = 64) -> 32 -> 16 -> 8\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Learning Rate and Epochs\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 2 # TRAIN FOR ONLY 2 EPOCHS (based on previous validation results)\n",
    "WEIGHT_DECAY = 1e-5 # Regularization\n",
    "\n",
    "# Initialize NCF model\n",
    "model = NCF(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    mf_dim=MF_DIM,\n",
    "    mlp_layers=MLP_LAYERS,\n",
    "    mlp_embedding_dim=MLP_EMBEDDING_DIM,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Loss Function (handles logits directly)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer WITH WEIGHT DECAY\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(\"Model, Criterion, and Optimizer initialized.\")\n",
    "# print(model) # Optional: Can uncomment to see architecture again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "print(\"\\n--- Starting NCF Training ---\")\n",
    "train_losses = []\n",
    "val_losses = [] # Basic validation loss (on positive samples only)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "\n",
    "    for users, items, labels in progress_bar:\n",
    "        # Move data to device\n",
    "        users, items, labels = users.to(device), items.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(users, items)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()}) # Show loss for current batch\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Training Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # --- Basic Validation ---\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for users, items in val_loader: # Val dataset only yields user, item\n",
    "            users, items = users.to(device), items.to(device)\n",
    "            # Create labels (all 1s for positive validation samples)\n",
    "            labels = torch.ones(users.size(0), device=device)\n",
    "            logits = model(users, items)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"--- NCF Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, marker='o', label='Training Loss')\n",
    "plt.plot(range(1, EPOCHS + 1), val_losses, marker='x', label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"BCEWithLogitsLoss\")\n",
    "plt.title(\"NCF Training History\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Evaluate NCF Model (Corrected WITH Imports)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Ensure project root is in sys.path ---\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "# -----------------------------------------\n",
    "\n",
    "# --- Import necessary functions/classes ---\n",
    "from src import config\n",
    "from src.data import preprocess # For time_based_split\n",
    "from src.evaluation.evaluator import RecEvaluator # <<< IMPORT RecEvaluator\n",
    "#------------------------------------------\n",
    "\n",
    "# --- Ensure necessary variables are defined (Check before proceeding) ---\n",
    "if 'model' not in locals(): raise NameError(\"NCF model 'model' not defined. Run training cell first.\")\n",
    "if 'user_id_map' not in locals(): raise NameError(\"'user_id_map' not defined. Run cell [3] first.\")\n",
    "if 'item_id_map' not in locals(): raise NameError(\"'item_id_map' not defined. Run cell [3] first.\")\n",
    "#-----------------------------------------------\n",
    "\n",
    "# --- Load or Recreate the CORRECT Time-Based Train/Test Split ---\n",
    "# Using Option 2: Recreate the split (ensure consistency with baselines)\n",
    "print(\"Recreating time-based split for evaluation...\")\n",
    "interactions_path_eval = config.PROCESSED_DATA_DIR / \"interactions_final.parquet\"\n",
    "if 'interactions_df' not in locals(): # Load if not already in memory from cell [2]\n",
    "    if not interactions_path_eval.exists():\n",
    "         raise FileNotFoundError(f\"Cannot find {interactions_path_eval}. Run preprocessing first.\")\n",
    "    interactions_df = pd.read_parquet(interactions_path_eval)\n",
    "\n",
    "TIME_THRESHOLD = 250 # <<< Make sure this is the same threshold used in 03_baseline_models!\n",
    "train_df_eval, test_df_eval = preprocess.time_based_split(\n",
    "    interactions_df=interactions_df,\n",
    "    user_col='id_student',\n",
    "    item_col='presentation_id',\n",
    "    time_col='last_interaction_date',\n",
    "    time_unit_threshold=TIME_THRESHOLD\n",
    ")\n",
    "print(f\"Time-based split recreated. Train: {train_df_eval.shape}, Test: {test_df_eval.shape}\")\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# --- Load Item Features ---\n",
    "items_df_path = config.PROCESSED_DATA_DIR / \"items_final.parquet\"\n",
    "if 'items_df' not in locals() or (isinstance(items_df, pd.DataFrame) and items_df.index.name != 'presentation_id'):\n",
    "     print(\"Loading items_df...\")\n",
    "     items_df = pd.read_parquet(items_df_path)\n",
    "     if 'presentation_id' in items_df.columns:\n",
    "        items_df = items_df.set_index('presentation_id')\n",
    "     else:\n",
    "        raise ValueError(\"Items DataFrame must have 'presentation_id' column.\")\n",
    "elif not isinstance(items_df, pd.DataFrame): # Check if it was somehow overwritten\n",
    "     raise TypeError(\"items_df is not a DataFrame.\")\n",
    "print(\"Items DataFrame ready.\")\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# --- Wrap the NCF model ---\n",
    "class NCFEvaluatorWrapper:\n",
    "    def __init__(self, ncf_model, user_map, item_map):\n",
    "        self.model = ncf_model\n",
    "        self.user_id_to_idx = user_map # Rename for clarity? user_map -> user_id_to_idx\n",
    "        self.item_id_to_idx = item_map # Rename for clarity? item_map -> item_id_to_idx\n",
    "        self.device = next(ncf_model.parameters()).device\n",
    "\n",
    "    def predict(self, user_id, item_ids):\n",
    "        \"\"\" Predicts scores for original user/item IDs \"\"\"\n",
    "        user_idx = self.user_id_to_idx.get(user_id)\n",
    "        if user_idx is None: return np.zeros(len(item_ids), dtype=np.float32) # Return float\n",
    "\n",
    "        # Map item IDs to indices, handling unknowns\n",
    "        item_idxs_eval = []\n",
    "        original_pos_map_eval = {} # Maps internal index back to original position\n",
    "        for i, iid in enumerate(item_ids):\n",
    "            item_idx = self.item_id_to_idx.get(iid)\n",
    "            if item_idx is not None: # Only include items known to the model\n",
    "                item_idxs_eval.append(item_idx)\n",
    "                original_pos_map_eval[item_idx] = i # Store original index\n",
    "\n",
    "        if not item_idxs_eval: return np.zeros(len(item_ids), dtype=np.float32) # Return float\n",
    "\n",
    "        user_tensor_eval = torch.tensor([user_idx] * len(item_idxs_eval), dtype=torch.long)\n",
    "        item_tensor_eval = torch.tensor(item_idxs_eval, dtype=torch.long)\n",
    "\n",
    "        # Use model's predict method which handles device and no_grad\n",
    "        scores_known_items = self.model.predict(user_tensor_eval, item_tensor_eval)\n",
    "\n",
    "        final_scores = np.zeros(len(item_ids), dtype=np.float32) # Initialize with float\n",
    "        for valid_idx, score in zip(item_idxs_eval, scores_known_items):\n",
    "            original_pos = original_pos_map_eval.get(valid_idx)\n",
    "            if original_pos is not None:\n",
    "                final_scores[original_pos] = float(score) # Ensure float\n",
    "\n",
    "        return final_scores\n",
    "\n",
    "    # --- ADD THESE METHODS ---\n",
    "    def get_known_items(self) -> set:\n",
    "        \"\"\"Returns the set of item IDs known during NCF training.\"\"\"\n",
    "        return set(self.item_id_to_idx.keys())\n",
    "\n",
    "    def get_known_users(self) -> set:\n",
    "        \"\"\"Returns the set of user IDs known during NCF training.\"\"\"\n",
    "        return set(self.user_id_to_idx.keys())\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# Wrap the *trained* NCF model (variable named 'model' from cell [5])\n",
    "ncf_eval_wrapper = NCFEvaluatorWrapper(model, user_id_map, item_id_map)\n",
    "print(\"NCF model wrapped for evaluator.\")\n",
    "\n",
    "# --- Initialize Evaluator and Evaluate ---\n",
    "if test_df_eval.empty:\n",
    "    print(\"\\nCannot evaluate NCF model: Test data (time-split) is empty.\")\n",
    "else:\n",
    "    print(f\"\\nInitializing evaluator with Train: {train_df_eval.shape}, Test: {test_df_eval.shape}\")\n",
    "    ncf_evaluator = RecEvaluator( # <<< RecEvaluator should now be defined\n",
    "        train_df=train_df_eval,\n",
    "        test_df=test_df_eval,\n",
    "        item_features_df=items_df,\n",
    "        user_col='id_student',\n",
    "        item_col='presentation_id',\n",
    "        k=config.TOP_K\n",
    "    )\n",
    "\n",
    "    # Evaluate the NCF model\n",
    "    print(\"\\n--- Starting Evaluation of NCF Model (Trained for 2 Epochs) ---\")\n",
    "    ncf_results = ncf_evaluator.evaluate_model(ncf_eval_wrapper, n_neg_samples=100)\n",
    "\n",
    "    print(\"\\nNCF Model Evaluation Results:\")\n",
    "    print(ncf_results)\n",
    "#-----------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
