{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math # Import math\n",
    "\n",
    "# Add project root to sys.path\n",
    "project_root = Path.cwd().parent # Should be RECSYS_FINAL\n",
    "src_path = project_root / \"src\"\n",
    "sys.path.append(str(project_root)) # Add project root for imports like 'src.config'\n",
    "\n",
    "# Import project modules\n",
    "from src import config\n",
    "from src.data import preprocess # For time_based_split\n",
    "from src.evaluation.evaluator import RecEvaluator # Import the evaluator class\n",
    "from src.models.popularity import PopularityRecommender # Import the model\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"Setup complete. Modules imported.\")\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Processed Data Dir: {config.PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed parquet files\n",
    "try:\n",
    "    interactions_df = pd.read_parquet(config.PROCESSED_DATA_DIR / \"interactions_final.parquet\")\n",
    "    users_df = pd.read_parquet(config.PROCESSED_DATA_DIR / \"users_final.parquet\")\n",
    "    items_df = pd.read_parquet(config.PROCESSED_DATA_DIR / \"items_final.parquet\") # Contains presentation_id as column\n",
    "    print(\"Processed data loaded successfully.\")\n",
    "    print(f\"Interactions shape: {interactions_df.shape}\")\n",
    "    print(f\"Users shape: {users_df.shape}\")\n",
    "    print(f\"Items shape: {items_df.shape}\")\n",
    "\n",
    "    # Set presentation_id as index for items_df if needed later (evaluator uses it)\n",
    "    if 'presentation_id' in items_df.columns:\n",
    "        items_df = items_df.set_index('presentation_id')\n",
    "        print(\"Set 'presentation_id' as index for items_df.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading processed files: {e}\")\n",
    "    print(\"Please ensure the preprocessing pipeline (run_preprocessing.py) has been run successfully.\")\n",
    "    # Stop execution or handle error\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during loading: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Display heads\n",
    "print(\"\\nInteractions Head:\\n\", interactions_df.head())\n",
    "print(\"\\nUsers Head:\\n\", users_df.head())\n",
    "print(\"\\nItems Head:\\n\", items_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3]: Time-Based Split (Using Threshold)\n",
    "\n",
    "time_col = 'last_interaction_date'\n",
    "user_col_in_df = 'id_student'      # Actual column name in interactions_df\n",
    "item_col_in_df = 'presentation_id' # Actual column name in interactions_df\n",
    "\n",
    "# --- Determine Threshold ---\n",
    "print(\"--- Determining Time Threshold ---\")\n",
    "print(interactions_df[time_col].describe(percentiles=[.75, .8, .85, .9, .95]))\n",
    "# Choose threshold based on percentiles (e.g., 80th percentile)\n",
    "# ***** REPLACE 229 WITH YOUR CHOSEN VALUE *****\n",
    "TIME_THRESHOLD = 250\n",
    "print(f\"Chosen Time Threshold: {TIME_THRESHOLD}\")\n",
    "print(\"--- End Threshold Determination ---\")\n",
    "\n",
    "\n",
    "# --- Perform Split ---\n",
    "if time_col not in interactions_df.columns:\n",
    "    raise ValueError(f\"Time column '{time_col}' not found in interactions data.\")\n",
    "if user_col_in_df not in interactions_df.columns:\n",
    "    raise ValueError(f\"User column '{user_col_in_df}' not found in interactions data.\")\n",
    "if item_col_in_df not in interactions_df.columns:\n",
    "    raise ValueError(f\"Item column '{item_col_in_df}' not found in interactions data.\")\n",
    "if not pd.api.types.is_numeric_dtype(interactions_df[time_col]):\n",
    "     raise TypeError(f\"Time column '{time_col}' must be numeric.\")\n",
    "\n",
    "train_df, test_df = preprocess.time_based_split(\n",
    "    interactions_df=interactions_df,\n",
    "    user_col=user_col_in_df,\n",
    "    item_col=item_col_in_df,\n",
    "    time_col=time_col,\n",
    "    time_unit_threshold=TIME_THRESHOLD # <<< Use the threshold\n",
    "    # split_ratio=None # Ensure split_ratio is not used\n",
    ")\n",
    "\n",
    "# --- Verify Split ---\n",
    "print(f\"\\nTrain shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "if not test_df.empty:\n",
    "    print(f\"Min time in Train: {train_df[time_col].min()}, Max time in Train: {train_df[time_col].max()}\")\n",
    "    print(f\"Min time in Test: {test_df[time_col].min()}, Max time in Test: {test_df[time_col].max()}\")\n",
    "    # Check user/item overlap\n",
    "    train_users_final = set(train_df[user_col_in_df].unique())\n",
    "    test_users_final = set(test_df[user_col_in_df].unique())\n",
    "    print(f\"Users in Train: {len(train_users_final)}, Users in Test: {len(test_users_final)}\")\n",
    "    print(f\"Users ONLY in Test: {len(test_users_final - train_users_final)}\") # Should be 0 after filtering in split func\n",
    "\n",
    "    train_items_final = set(train_df[item_col_in_df].unique())\n",
    "    test_items_final = set(test_df[item_col_in_df].unique())\n",
    "    print(f\"Items in Train: {len(train_items_final)}, Items in Test: {len(test_items_final)}\")\n",
    "    print(f\"Items ONLY in Test: {len(test_items_final - train_items_final)}\") # Should be 0 after filtering in split func\n",
    "\n",
    "else:\n",
    "    print(\"Warning: Test DataFrame is empty!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Train Popularity Model\n",
    "\n",
    "# Initialize and train the Popularity model\n",
    "# Ensure the item_col matches the column name in train_df and test_df\n",
    "pop_model = PopularityRecommender(\n",
    "    user_col='id_student',          # <<< Use the actual user column name\n",
    "    item_col='presentation_id',     # <<< Use the actual item column name\n",
    "    score_col='implicit_feedback'\n",
    ")\n",
    "\n",
    "# Fit the model using the training data\n",
    "pop_model.fit(train_df)\n",
    "\n",
    "# (Optional) Test prediction for a sample user/items\n",
    "if not test_df.empty:\n",
    "    sample_user = test_df['id_student'].iloc[0]\n",
    "    sample_items_all = items_df.index.tolist() # Get all unique item IDs from items_df index\n",
    "    sample_items_subset = np.random.choice(sample_items_all, min(10, len(sample_items_all)), replace=False).tolist() # Ensure not sampling more than available\n",
    "    print(f\"\\nTesting prediction for user {sample_user} on items: {sample_items_subset}\")\n",
    "    scores = pop_model.predict(sample_user, sample_items_subset)\n",
    "    print(\"Scores (Popularity):\", scores)\n",
    "else:\n",
    "    print(\"\\nSkipping prediction test as test_df is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Evaluate Popularity Model\n",
    "\n",
    "# Initialize the evaluator\n",
    "# Ensure items_df has presentation_id as index before passing\n",
    "if test_df.empty:\n",
    "     print(\"\\nCannot evaluate model: Test data is empty.\")\n",
    "elif items_df.index.name != 'presentation_id':\n",
    "     print(\"\\nError: items_df must have 'presentation_id' set as index for evaluator.\")\n",
    "else:\n",
    "    evaluator = RecEvaluator(\n",
    "        train_df=train_df,\n",
    "        test_df=test_df,\n",
    "        item_features_df=items_df, # Pass items_df with index set\n",
    "        user_col='id_student',     # <<< Use the actual user column name\n",
    "        item_col='presentation_id',# <<< Use the actual item column name\n",
    "        k=config.TOP_K             # Use K from config\n",
    "    )\n",
    "\n",
    "    # Evaluate the popularity model\n",
    "    # Using n_neg_samples can speed things up significantly for evaluation if needed\n",
    "    # Set n_neg_samples=100 for faster (approximate) evaluation, or None for full evaluation\n",
    "    print(\"\\n--- Starting Evaluation of Popularity Model ---\")\n",
    "    pop_results = evaluator.evaluate_model(pop_model, n_neg_samples=100)\n",
    "\n",
    "    print(\"\\nPopularity Model Evaluation Results:\")\n",
    "    print(pop_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Train ItemCF Model\n",
    "\n",
    "# Import the model\n",
    "from src.models.item_cf import ItemCFRecommender\n",
    "\n",
    "# Initialize and train the ItemCF model\n",
    "itemcf_model = ItemCFRecommender(\n",
    "    user_col='id_student',          # Use the actual user column name\n",
    "    item_col='presentation_id',     # Use the actual item column name\n",
    "    score_col='implicit_feedback'\n",
    ")\n",
    "\n",
    "# Fit the model using the training data\n",
    "itemcf_model.fit(train_df)\n",
    "\n",
    "# (Optional) Test prediction for a sample user/items\n",
    "if not test_df.empty:\n",
    "    # Use the same sample user as before or pick a new one\n",
    "    sample_user_id = test_df['id_student'].iloc[0]\n",
    "    # Ensure the user exists in the model's mapping\n",
    "    if sample_user_id in itemcf_model.user_id_to_idx:\n",
    "        # Get items the user interacted with in train and test for context\n",
    "        user_train_interactions = train_df[train_df['id_student'] == sample_user_id]['presentation_id'].tolist()\n",
    "        user_test_interactions = test_df[test_df['id_student'] == sample_user_id]['presentation_id'].tolist()\n",
    "        print(f\"\\n--- ItemCF Prediction Test ---\")\n",
    "        print(f\"Sample User ID: {sample_user_id}\")\n",
    "        print(f\" User's Training Items: {user_train_interactions}\")\n",
    "        print(f\" User's Test Items (Ground Truth): {user_test_interactions}\")\n",
    "\n",
    "        # Predict scores for the test items and a few others\n",
    "        sample_items_all = items_df.index.tolist()\n",
    "        items_to_predict = user_test_interactions + np.random.choice(sample_items_all, 5, replace=False).tolist()\n",
    "        items_to_predict = list(set(items_to_predict)) # Ensure unique items\n",
    "\n",
    "        print(f\" Predicting for Items: {items_to_predict}\")\n",
    "        scores = itemcf_model.predict(sample_user_id, items_to_predict)\n",
    "        print(\" Predicted Scores:\", scores)\n",
    "        print(\"--- End Prediction Test ---\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Sample user {sample_user_id} not found in ItemCF model training data.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping ItemCF prediction test as test_df is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Evaluate ItemCF Model\n",
    "\n",
    "# Evaluate the ItemCF model using the same evaluator instance\n",
    "if 'evaluator' in locals() and evaluator is not None: # Check if evaluator exists\n",
    "    print(\"\\n--- Starting Evaluation of ItemCF Model ---\")\n",
    "    itemcf_results = evaluator.evaluate_model(itemcf_model, n_neg_samples=100) # Use negative sampling\n",
    "\n",
    "    print(\"\\nItemCF Model Evaluation Results:\")\n",
    "    print(itemcf_results)\n",
    "\n",
    "elif test_df.empty:\n",
    "    print(\"\\nCannot evaluate model: Test data is empty.\")\n",
    "else:\n",
    "     print(\"\\nError: Evaluator not initialized. Please run Cell [5] successfully first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10] - Train ItemCF Model\n",
    "\n",
    "# Import the model\n",
    "from src.models.item_cf import ItemCFRecommender\n",
    "\n",
    "print(\"\\n--- Training ItemCF Model ---\")\n",
    "\n",
    "# Initialize and train the ItemCF model\n",
    "itemcf_model = ItemCFRecommender(\n",
    "    user_col='id_student',          # Use the actual user column name from interactions_df\n",
    "    item_col='presentation_id',     # Use the actual item column name from interactions_df\n",
    "    score_col='implicit_feedback'\n",
    ")\n",
    "\n",
    "# Fit the model using the training data\n",
    "# This might take a moment as it calculates the similarity matrix\n",
    "itemcf_model.fit(train_df)\n",
    "\n",
    "# (Optional) Test prediction for a sample user/items\n",
    "if not test_df.empty:\n",
    "    # Use the same sample user as before or pick a new one from the test set\n",
    "    sample_user_id = test_df['id_student'].iloc[0] # Example: first user in test set\n",
    "\n",
    "    # Ensure the user exists in the model's mapping\n",
    "    if sample_user_id in itemcf_model.user_id_to_idx:\n",
    "        # Get items the user interacted with in train and test for context\n",
    "        user_train_interactions = train_df[train_df['id_student'] == sample_user_id]['presentation_id'].tolist()\n",
    "        user_test_interactions = test_df[test_df['id_student'] == sample_user_id]['presentation_id'].tolist()\n",
    "        print(f\"\\n--- ItemCF Prediction Test ---\")\n",
    "        print(f\"Sample User ID: {sample_user_id}\")\n",
    "        print(f\" User's Training Items: {user_train_interactions}\")\n",
    "        print(f\" User's Test Items (Ground Truth): {user_test_interactions}\")\n",
    "\n",
    "        # Predict scores for the test items and a few others\n",
    "        sample_items_all = items_df.index.tolist()\n",
    "        items_to_predict = user_test_interactions + np.random.choice(sample_items_all, 5, replace=False).tolist()\n",
    "        items_to_predict = list(set(items_to_predict)) # Ensure unique items\n",
    "\n",
    "        print(f\" Predicting for Items: {items_to_predict}\")\n",
    "        scores = itemcf_model.predict(sample_user_id, items_to_predict)\n",
    "        print(\" Predicted Scores:\", scores)\n",
    "        # Display scores alongside item IDs for better readability\n",
    "        scored_preds = sorted(list(zip(items_to_predict, scores)), key=lambda x: x[1], reverse=True)\n",
    "        print(\" Predicted Scores (Sorted):\", scored_preds)\n",
    "        print(\"--- End Prediction Test ---\")\n",
    "    else:\n",
    "        print(f\"\\nSample user {sample_user_id} not found in ItemCF model training data (this shouldn't happen if test set was filtered correctly).\")\n",
    "else:\n",
    "    print(\"\\nSkipping ItemCF prediction test as test_df is empty.\")\n",
    "\n",
    "print(\"\\n--- Finished Training ItemCF Model ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11] - Evaluate ItemCF Model\n",
    "\n",
    "# Evaluate the ItemCF model using the same evaluator instance\n",
    "if 'evaluator' in locals() and evaluator is not None: # Check if evaluator exists\n",
    "    print(\"\\n--- Starting Evaluation of ItemCF Model ---\")\n",
    "    itemcf_results = evaluator.evaluate_model(itemcf_model, n_neg_samples=100) # Use negative sampling\n",
    "\n",
    "    print(\"\\nItemCF Model Evaluation Results:\")\n",
    "    print(itemcf_results)\n",
    "\n",
    "elif test_df.empty:\n",
    "    print(\"\\nCannot evaluate ItemCF model: Test data is empty.\")\n",
    "else:\n",
    "     print(\"\\nError: Evaluator not initialized. Please run the cell that initializes 'evaluator' successfully first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [12] - Train ALS Model\n",
    "\n",
    "# Import the model\n",
    "from src.models.matrix_factorization import ImplicitALSWrapper\n",
    "\n",
    "print(\"\\n--- Training Implicit ALS Model ---\")\n",
    "\n",
    "# Initialize and train the ALS model\n",
    "# Adjust hyperparameters as needed (these are examples)\n",
    "als_model = ImplicitALSWrapper(\n",
    "    user_col='id_student',\n",
    "    item_col='presentation_id',\n",
    "    score_col='implicit_feedback',\n",
    "    factors=50,           # Latent factors\n",
    "    regularization=0.05,  # Regularization\n",
    "    iterations=25,        # Iterations\n",
    "    random_state=config.RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Fit the model using the training data\n",
    "# This will take longer than Popularity or ItemCF\n",
    "als_model.fit(train_df)\n",
    "\n",
    "# (Optional) Test prediction for a sample user/items\n",
    "if not test_df.empty:\n",
    "    sample_user_id = test_df['id_student'].iloc[0]\n",
    "    if sample_user_id in als_model.user_id_to_idx:\n",
    "        user_train_interactions = train_df[train_df['id_student'] == sample_user_id]['presentation_id'].tolist()\n",
    "        user_test_interactions = test_df[test_df['id_student'] == sample_user_id]['presentation_id'].tolist()\n",
    "        print(f\"\\n--- ALS Prediction Test ---\")\n",
    "        print(f\"Sample User ID: {sample_user_id}\")\n",
    "        print(f\" User's Training Items: {user_train_interactions}\")\n",
    "        print(f\" User's Test Items (Ground Truth): {user_test_interactions}\")\n",
    "\n",
    "        sample_items_all = items_df.index.tolist()\n",
    "        items_to_predict = user_test_interactions + np.random.choice(sample_items_all, 5, replace=False).tolist()\n",
    "        items_to_predict = list(set(items_to_predict))\n",
    "\n",
    "        print(f\" Predicting for Items: {items_to_predict}\")\n",
    "        scores = als_model.predict(sample_user_id, items_to_predict)\n",
    "        scored_preds = sorted(list(zip(items_to_predict, scores)), key=lambda x: x[1], reverse=True)\n",
    "        print(\" Predicted Scores (Sorted):\", scored_preds)\n",
    "        print(\"--- End Prediction Test ---\")\n",
    "    else:\n",
    "        print(f\"\\nSample user {sample_user_id} not found in ALS model training data.\")\n",
    "else:\n",
    "    print(\"\\nSkipping ALS prediction test as test_df is empty.\")\n",
    "\n",
    "print(\"\\n--- Finished Training Implicit ALS Model ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [13] - Evaluate ALS Model\n",
    "\n",
    "# Evaluate the ALS model using the same evaluator instance\n",
    "if 'evaluator' in locals() and evaluator is not None:\n",
    "    print(\"\\n--- Starting Evaluation of Implicit ALS Model ---\")\n",
    "    als_results = evaluator.evaluate_model(als_model, n_neg_samples=100) # Use negative sampling\n",
    "\n",
    "    print(\"\\nImplicit ALS Model Evaluation Results:\")\n",
    "    print(als_results)\n",
    "\n",
    "elif test_df.empty:\n",
    "    print(\"\\nCannot evaluate ALS model: Test data is empty.\")\n",
    "else:\n",
    "     print(\"\\nError: Evaluator not initialized. Please run the cell that initializes 'evaluator' successfully first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
