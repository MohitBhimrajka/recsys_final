# OULAD MOOC Course Recommendation System

## Project Overview

This project implements a course recommendation system using the Open University Learning Analytics Dataset (OULAD). The goal is to recommend relevant course presentations (`code_module` + `code_presentation`) to students based on their historical interactions, demographics, and course characteristics.

Various recommendation techniques were explored, including baselines (Popularity, ItemCF), matrix factorization (ALS), and neural approaches (NCF, Hybrid NCF). The system preprocesses raw OULAD data, trains selected models, and evaluates them using a time-based split and standard ranking metrics. **Item-Based Collaborative Filtering (ItemCF) was found to be the most effective model for this dataset and preprocessing setup.**

The project also includes a demonstration API (FastAPI) and a simple interactive frontend (React+TypeScript+Vite) to showcase the ItemCF model's recommendations.

## Dataset

*   **Source:** Open University Learning Analytics Dataset (OULAD)
*   **Files Used:** `assessments.csv`, `courses.csv`, `studentAssessment.csv`, `studentInfo.csv`, `studentRegistration.csv`, `studentVle.csv`, `vle.csv`
*   **Location:** Raw CSV files should be placed in the `data/raw/` directory after downloading.

## Features

*   **Comprehensive Preprocessing:** Cleans OULAD data, handles missing values, filters sparse interactions, and engineers features.
*   **Implicit Feedback:** Uses `log1p(total_clicks)` as the primary engagement signal.
*   **Multiple Models:** Implements and compares Popularity, ItemCF, ALS, NCF, and a Hybrid NCF model incorporating content features.
*   **Time-Based Evaluation:** Employs a realistic time-based split (`last_interaction_date <= 250`) for training and testing.
*   **Standard Metrics:** Evaluates models using Precision@10, Recall@10, and NDCG@10.
*   **Modular Code:** Structured Python code (`src/`) for data handling, modeling, evaluation, and pipelines.
*   **Database Integration (Optional):** Scripts to load processed data into a PostgreSQL database.
*   **API & Frontend:** Includes a FastAPI backend and a React frontend for demonstrating the ItemCF model.

## Project Structure

```
recsys_final/
│
├── data/
│   ├── raw/              # Original OULAD CSVs (Needs to be downloaded)
│   └── processed/        # Processed data (Generated by preprocessing script)
│
├── notebooks/            # Jupyter notebooks for EDA, development, experimentation
│
├── src/                  # Core Python library for the project
│   ├── config.py         # Configuration (paths, DB URI, parameters)
│   ├── data/             # Data loading, preprocessing, feature engineering, PyTorch Datasets
│   ├── database/         # Database schema (SQLAlchemy), connection, loading scripts
│   ├── evaluation/       # Evaluation metrics and protocol (RecEvaluator)
│   ├── models/           # Recommender model implementations (Base, ItemCF, NCF, etc.)
│   ├── pipelines/        # End-to-end scripts (preprocessing, training, evaluation)
│   └── ...               # Other helper modules if any
│
├── api/                  # FastAPI backend application
│   ├── app/              # FastAPI app code (main, routers, services, etc.)
│   └── requirements.txt  # API specific dependencies
│
├── frontend/             # React+TypeScript frontend application
│   ├── src/              # Frontend source code (components, pages, services)
│   ├── index.html        # Main HTML entry point
│   └── ...               # Other frontend config files (vite, tailwind, etc.)
│
├── reports/              # EDA summary, interim, and final project reports (Markdown)
│
├── saved_models/         # Trained model artifacts (Generated by training script)
│
├── tests/                # Pytest tests for various modules
│
├── .env                  # Local environment variables (DB credentials - DO NOT COMMIT)
├── .env.example          # Template for .env
├── .gitignore            # Specifies intentionally untracked files
├── requirements.txt      # Core Python project dependencies
├── pytest.ini            # Pytest configuration
└── README.md             # This file
```

## Setup Instructions

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/MohitBhimrajka/recsys_final
    cd recsys_final
    ```

2.  **Create and activate a virtual environment** (Python 3.9+ recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3.  **Install Core Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Install API Dependencies:**
    ```bash
    pip install -r api/requirements.txt
    ```

5.  **Download OULAD Data:**
    *   The OULAD dataset needs to be downloaded separately.
    *   **Source:** You can typically find it by searching for "Open University Learning Analytics Dataset Download" or visiting data repositories like Kaggle or the original Open University source if available. A common source is: [https://www.kaggle.com/datasets/anlgrbz/student-demographics-online-education-dataoulad](https://www.kaggle.com/datasets/anlgrbz/student-demographics-online-education-dataoulad) (Check for the most current link).
    *   **Action:** Download the dataset archive (usually a `.zip` file).
    *   **Action:** Create the `data/raw/` directory within your `recsys_final` project folder if it doesn't exist.
    *   **Action:** Extract the **seven required CSV files** (`assessments.csv`, `courses.csv`, `studentAssessment.csv`, `studentInfo.csv`, `studentRegistration.csv`, `studentVle.csv`, `vle.csv`) directly into the `recsys_final/data/raw/` directory. Do not place them in a subfolder within `raw/`.

6.  **Set up PostgreSQL Database (Optional):**
    *   Needed only if you plan to run `src/database/load_to_db.py`.
    *   Ensure PostgreSQL is installed and running.
    *   Create a database (e.g., `oulad_recsys`) and a user with privileges.

7.  **Configure Database Connection (Optional):**
    *   Copy `.env.example` to `.env`: `cp .env.example .env`
    *   Edit `.env` and fill in your PostgreSQL `DB_USER`, `DB_PASSWORD`, `DB_HOST`, `DB_PORT`, `DB_NAME`.

8.  **Verify Setup:**
    *   Run the configuration check (prints info, checks if raw data files are found in `data/raw/`):
        ```bash
        python src/config.py
        ```

## Usage Workflow

Follow these steps to run the project pipelines:

1.  **Exploratory Data Analysis (EDA):**
    *   Start Jupyter Lab: `jupyter lab`
    *   Navigate to `notebooks/` and run `01_eda.ipynb` to understand the data. Review `reports/eda_summary.md`.

2.  **Data Preprocessing:**
    *   Ensure raw CSVs are in `data/raw/`.
    *   Run the preprocessing script:
        ```bash
        python src/pipelines/run_preprocessing.py
        ```
    *   This generates `interactions_final.parquet`, `users_final.parquet`, and `items_final.parquet` in `data/processed/`.

3.  **Database Setup & Loading (Optional):**
    *   Ensure DB is running and `.env` is configured.
    *   **(First Time Only)** Setup schema. **Warning:** Check `DROP_EXISTING_TABLES` flag in the script before running!
        ```bash
        python src/pipelines/setup_database.py
        ```
    *   Load processed data into the database:
        ```bash
        python src/database/load_to_db.py
        ```

4.  **Model Training:**
    *   Train specific models using the processed interactions. Models are saved to `saved_models/`.
    *   **Examples:**
        ```bash
        # Train Popularity model
        python src/pipelines/train.py --model-name Popularity

        # Train ItemCF model (used in demo)
        python src/pipelines/train.py --model-name ItemCF

        # Train ALS model with specific parameters
        python src/pipelines/train.py --model-name ALS --factors 100 --iterations 30 --regularization 0.05

        # Train NCF model with specific parameters
        python src/pipelines/train.py --model-name NCF --epochs 15 --lr 0.001 --batch-size 2048

        # Train Hybrid model (requires item features)
        python src/pipelines/train.py --model-name Hybrid --epochs 15 --lr 0.001 --batch-size 512
        ```

5.  **Model Evaluation:**
    *   Evaluate a *saved* model artifact using the time-based split.
    *   **Example:**
        ```bash
        # Evaluate the saved ItemCF model
        python src/pipelines/evaluate.py --model-path saved_models/ItemCF.pkl --k 10 --neg-samples 100

        # Evaluate a saved NCF model and save metrics
        python src/pipelines/evaluate.py --model-path saved_models/NCF_epochs15_lr0.001.pt --k 10 --neg-samples 100 --metrics-output-path reports/evaluation/ncf_results.json
        ```
    *   `--k`: Rank threshold for metrics.
    *   `--neg-samples`: Number of negative items to sample per positive test item for evaluation (0 for full ranking, potentially slow).
    *   `--metrics-output-path`: Optional path to save results as JSON.

6.  **Run API Server:**
    *   Navigate to the API directory: `cd api`
    *   Start the server (with auto-reload):
        ```bash
        uvicorn app.main:app --reload --port 8000
        ```
    *   Access API docs at `http://localhost:8000/docs`.

7.  **Run Frontend Application:**
    *   Navigate to the frontend directory: `cd frontend`
    *   Install dependencies (if not already done): `npm install`
    *   Start the development server: `npm run dev`
    *   Access the application in your browser, usually at `http://localhost:5173`.

## Models Implemented

*   **Popularity:** Non-personalized baseline recommending items with the highest overall `implicit_feedback` sum.
*   **ItemCF (Item-Based Collaborative Filtering):** Recommends items similar (cosine similarity) to those a user interacted with positively, based on co-interaction patterns. **(Used in Demo)**
*   **ALS (Alternating Least Squares):** Matrix factorization technique implemented via the `implicit` library to find latent user/item factors.
*   **NCF (Neural Collaborative Filtering):** Deep learning model combining matrix factorization (GMF) and MLP pathways to capture complex user-item interactions.
*   **Hybrid NCF:** Extends NCF by incorporating pre-computed item content features (like VLE activity proportions, length) via a dedicated `ContentEncoder` MLP.

## Evaluation Summary

*   **Split:** Time-based split on `last_interaction_date <= 250`.
*   **Metrics:** Precision@10, Recall@10, NDCG@10.
*   **Key Result:** ItemCF demonstrated the strongest performance, particularly in Recall@10 (0.9781) and NDCG@10 (0.6153), suggesting item co-occurrence patterns are highly predictive in this dataset context.