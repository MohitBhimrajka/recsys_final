{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Add project root to sys.path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src import config\n",
    "from src.data.dataset import HybridDataset, create_mappings_and_unique_ids # <<< Import HybridDataset\n",
    "from src.models.hybrid import HybridNCF # Import the Hybrid model\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"Setup complete. Modules imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Load Processed Data\n",
    "interactions_path = config.PROCESSED_DATA_DIR / \"interactions_final.parquet\"\n",
    "users_path = config.PROCESSED_DATA_DIR / \"users_final.parquet\"\n",
    "items_path = config.PROCESSED_DATA_DIR / \"items_final.parquet\"\n",
    "\n",
    "try:\n",
    "    interactions_df = pd.read_parquet(interactions_path)\n",
    "    users_features_df = pd.read_parquet(users_path) # User features (not directly used by HybridNCF yet)\n",
    "    item_features_df = pd.read_parquet(items_path)   # Item features (presentation_id as col)\n",
    "    print(\"Processed data loaded successfully.\")\n",
    "    print(f\"Interactions shape: {interactions_df.shape}\")\n",
    "    print(f\"Users shape: {users_features_df.shape}\")\n",
    "    print(f\"Items shape: {item_features_df.shape}\")\n",
    "\n",
    "    # IMPORTANT: Ensure item_features_df has 'presentation_id' as index for the dataset\n",
    "    if 'presentation_id' in item_features_df.columns:\n",
    "        item_features_df = item_features_df.set_index('presentation_id')\n",
    "        print(\"Set 'presentation_id' as index for item_features_df.\")\n",
    "    elif item_features_df.index.name != 'presentation_id':\n",
    "         raise ValueError(\"item_features_df must have 'presentation_id' as index or column.\")\n",
    "\n",
    "    # Store item feature dimension\n",
    "    ITEM_FEATURE_DIM = item_features_df.shape[1]\n",
    "    print(f\"Item feature dimension: {ITEM_FEATURE_DIM}\")\n",
    "\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading processed files: {e}\")\n",
    "    print(\"Please ensure the preprocessing pipeline (run_preprocessing.py) has run successfully.\")\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during loading: {e}\")\n",
    "    raise e\n",
    "\n",
    "print(\"\\nInteractions Head:\\n\", interactions_df.head(3))\n",
    "print(\"\\nItem Features Head:\\n\", item_features_df.head(3))\n",
    "\n",
    "# Drop constant columns from item features if they exist (e.g., all zeros)\n",
    "# These provide no information for the MLP\n",
    "const_cols = item_features_df.columns[item_features_df.nunique() <= 1]\n",
    "if len(const_cols) > 0:\n",
    "    print(f\"\\nDropping constant item feature columns: {const_cols.tolist()}\")\n",
    "    item_features_df = item_features_df.drop(columns=const_cols)\n",
    "    ITEM_FEATURE_DIM = item_features_df.shape[1]\n",
    "    print(f\"Updated item feature dimension: {ITEM_FEATURE_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Create Mappings and Hybrid Dataset\n",
    "\n",
    "USER_COL = 'id_student'\n",
    "ITEM_COL = 'presentation_id' # This is the index name now in item_features_df\n",
    "\n",
    "# Create mappings from original IDs based on interactions data\n",
    "user_id_map, item_id_map, unique_users, unique_items = create_mappings_and_unique_ids(\n",
    "    interactions_df, USER_COL, ITEM_COL\n",
    ")\n",
    "n_users = len(unique_users)\n",
    "n_items = len(unique_items)\n",
    "\n",
    "print(f\"Number of unique users: {n_users}\")\n",
    "print(f\"Number of unique items: {n_items}\")\n",
    "\n",
    "# Ensure item_features_df covers all items in the map\n",
    "items_in_map_set = set(item_id_map.keys())\n",
    "items_in_features_set = set(item_features_df.index)\n",
    "if not items_in_map_set.issubset(items_in_features_set):\n",
    "    missing = items_in_map_set - items_in_features_set\n",
    "    raise ValueError(f\"{len(missing)} items from interactions are missing in item_features_df. E.g.: {list(missing)[:5]}\")\n",
    "if items_in_features_set != items_in_map_set:\n",
    "     print(f\"Warning: {len(items_in_features_set - items_in_map_set)} items in features_df are not in interactions_df.\")\n",
    "\n",
    "\n",
    "# Split interactions for train/validation (simple random split for dev)\n",
    "train_interactions, val_interactions = train_test_split(\n",
    "    interactions_df, test_size=0.1, random_state=config.RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Create Hybrid Datasets\n",
    "# Pass the (potentially column-filtered) item_features_df\n",
    "train_dataset_hybrid = HybridDataset(\n",
    "    interactions_df=train_interactions,\n",
    "    item_features_df=item_features_df,\n",
    "    all_item_ids=item_features_df.index.tolist(),\n",
    "    user_id_map=user_id_map,\n",
    "    item_id_map=item_id_map,\n",
    "    user_col=USER_COL,\n",
    "    item_col=ITEM_COL,\n",
    "    num_negatives=4\n",
    ")\n",
    "\n",
    "val_dataset_hybrid = HybridDataset(\n",
    "    interactions_df=val_interactions,\n",
    "    item_features_df=item_features_df,\n",
    "    all_item_ids=item_features_df.index.tolist(),\n",
    "    user_id_map=user_id_map,\n",
    "    item_id_map=item_id_map,\n",
    "    user_col=USER_COL,\n",
    "    item_col=ITEM_COL,\n",
    "    num_negatives=0\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 512 # Adjust based on memory\n",
    "train_loader_hybrid = DataLoader(train_dataset_hybrid, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader_hybrid = DataLoader(val_dataset_hybrid, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"\\nHybrid DataLoaders created. Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Test a batch\n",
    "print(\"\\nSample batch from Hybrid Train DataLoader:\")\n",
    "for batch in train_loader_hybrid:\n",
    "    users, items, feats, labels = batch\n",
    "    print(\" Users shape:\", users.shape)\n",
    "    print(\" Items shape:\", items.shape)\n",
    "    print(\" Feats shape:\", feats.shape) # Should be (BATCH_SIZE, ITEM_FEATURE_DIM)\n",
    "    print(\" Labels shape:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === New Cell: Instantiate and Train HybridNCFRecommender ===\n",
    "from src.models.hybrid import HybridNCFRecommender # Import the wrapper\n",
    "\n",
    "# Define hyperparameters for the wrapper\n",
    "CF_EMBEDDING_DIM_WRAP = 32\n",
    "CONTENT_EMBEDDING_DIM_WRAP = 16\n",
    "CONTENT_ENCODER_HIDDEN_WRAP = [32, 16]\n",
    "FINAL_MLP_LAYERS_WRAP = [64, 32, 16]\n",
    "DROPOUT_WRAP = 0.2\n",
    "LEARNING_RATE_WRAP = 0.001\n",
    "EPOCHS_WRAP = 10 # Match previous training\n",
    "WEIGHT_DECAY_WRAP = 1e-5\n",
    "BATCH_SIZE_WRAP = 512 # Match previous batch size\n",
    "NUM_NEGATIVES_WRAP = 4 # Match previous negative samples\n",
    "\n",
    "print(\"\\n--- Initializing HybridNCFRecommender ---\")\n",
    "hybrid_recommender = HybridNCFRecommender(\n",
    "    user_col=USER_COL, # Defined earlier\n",
    "    item_col=ITEM_COL, # Defined earlier\n",
    "    cf_embedding_dim=CF_EMBEDDING_DIM_WRAP,\n",
    "    content_embedding_dim=CONTENT_EMBEDDING_DIM_WRAP,\n",
    "    content_encoder_hidden_dims=CONTENT_ENCODER_HIDDEN_WRAP,\n",
    "    final_mlp_layers=FINAL_MLP_LAYERS_WRAP,\n",
    "    dropout=DROPOUT_WRAP,\n",
    "    learning_rate=LEARNING_RATE_WRAP,\n",
    "    epochs=EPOCHS_WRAP,\n",
    "    batch_size=BATCH_SIZE_WRAP,\n",
    "    num_negatives=NUM_NEGATIVES_WRAP,\n",
    "    weight_decay=WEIGHT_DECAY_WRAP,\n",
    "    device='auto'\n",
    ")\n",
    "\n",
    "# Train the model using the 'fit' method\n",
    "# Pass interactions data AND the item features DataFrame\n",
    "print(\"\\n--- Training HybridNCFRecommender ---\")\n",
    "# Ensure interactions_df, item_features_df are defined and correct\n",
    "# Fit on the full interactions data intended for this model instance\n",
    "# hybrid_recommender.fit(train_interactions, item_features_df) # Option 1: Fit on dev split\n",
    "hybrid_recommender.fit(interactions_df, item_features_df)      # Option 2: Fit on full data\n",
    "\n",
    "\n",
    "print(\"\\n--- HybridNCFRecommender Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] â€“ Evaluate Hybrid Model (Corrected WITH Wrapper)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Ensure project root is in sys.path ---\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "# -----------------------------------------\n",
    "\n",
    "# --- Import necessary functions/classes ---\n",
    "from src import config\n",
    "from src.data import preprocess  # For time_based_split\n",
    "from src.evaluation.evaluator import RecEvaluator\n",
    "# -----------------------------------------\n",
    "\n",
    "# --- Ensure necessary variables/data are defined ---\n",
    "# --- MODIFIED CHECK: Check for the wrapper instance ---\n",
    "if 'hybrid_recommender' not in locals():\n",
    "    raise NameError(\"HybridNCFRecommender instance 'hybrid_recommender' not defined. Run the training cell first.\")\n",
    "# ----------------------------------------------------\n",
    "if 'user_id_map' not in locals(): raise NameError(\"'user_id_map' not defined. Run cell [3] first.\")\n",
    "if 'item_id_map' not in locals(): raise NameError(\"'item_id_map' not defined. Run cell [3] first.\")\n",
    "# --- MODIFIED CHECK: Use item_features_df loaded earlier ---\n",
    "if 'item_features_df' not in locals() or not isinstance(item_features_df, pd.DataFrame):\n",
    "     raise NameError(\"'item_features_df' not defined or not a DataFrame. Run cell [2] first.\")\n",
    "if item_features_df.index.name != 'presentation_id': # Check index on the correct variable\n",
    "    raise ValueError(\"item_features_df (from cell [2]) must have 'presentation_id' as index.\")\n",
    "# -----------------------------------------------\n",
    "\n",
    "# --- Load/Recreate the CORRECT Time-Based Train/Test Split ---\n",
    "# (This section remains the same)\n",
    "print(\"Loading/Recreating time-based split for evaluation...\")\n",
    "# Use interactions_df if already loaded, otherwise load it\n",
    "if 'interactions_df' not in locals() or not isinstance(interactions_df, pd.DataFrame):\n",
    "    interactions_path_eval = config.PROCESSED_DATA_DIR / \"interactions_final.parquet\"\n",
    "    if not interactions_path_eval.exists():\n",
    "         raise FileNotFoundError(f\"Cannot find {interactions_path_eval}. Run preprocessing first.\")\n",
    "    interactions_df_eval = pd.read_parquet(interactions_path_eval) # Use a different name\n",
    "else:\n",
    "    interactions_df_eval = interactions_df # Use the one already loaded\n",
    "\n",
    "# --- USE CONFIG VALUE ---\n",
    "TIME_THRESHOLD = config.TIME_SPLIT_THRESHOLD\n",
    "train_df_eval, test_df_eval = preprocess.time_based_split(\n",
    "    interactions_df=interactions_df_eval,\n",
    "    user_col='id_student',\n",
    "    item_col='presentation_id',\n",
    "    time_col='last_interaction_date',\n",
    "    time_unit_threshold=TIME_THRESHOLD\n",
    ")\n",
    "print(f\"Time-based split ready. Train: {train_df_eval.shape}, Test: {test_df_eval.shape}\")\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# --- Item Features are already loaded in item_features_df from cell [2] ---\n",
    "print(\"Using item_features_df loaded in cell [2].\")\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# --- NO WRAPPER NEEDED HERE - Model is already wrapped ---\n",
    "# --- (Delete the old HybridEvaluatorWrapper class definition if it's still here) ---\n",
    "# --- (Delete the old hybrid_eval_wrapper = ... line if it's still here) ---\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Using the trained 'hybrid_recommender' instance directly.\")\n",
    "\n",
    "\n",
    "# --- Initialize Evaluator and Evaluate ---\n",
    "if test_df_eval.empty:\n",
    "    print(\"\\nCannot evaluate Hybrid model: Test data (time-split) is empty.\")\n",
    "# --- Use item_features_df loaded from cell [2] ---\n",
    "elif item_features_df.index.name != 'presentation_id':\n",
    "    print(\"\\nError: item_features_df must have 'presentation_id' set as index for evaluator.\")\n",
    "else:\n",
    "    print(f\"\\nInitializing evaluator with Train: {train_df_eval.shape}, Test: {test_df_eval.shape}\")\n",
    "    hybrid_evaluator = RecEvaluator(\n",
    "        train_df=train_df_eval,\n",
    "        test_df=test_df_eval,\n",
    "        # --- Pass the correctly loaded/indexed item_features_df ---\n",
    "        item_features_df=item_features_df,\n",
    "        user_col='id_student',\n",
    "        item_col='presentation_id',\n",
    "        k=config.TOP_K\n",
    "    )\n",
    "\n",
    "    # --- MODIFIED EVALUATION CALL: Use the wrapper instance ---\n",
    "    print(\"\\n--- Starting Evaluation of HybridNCFRecommender ---\")\n",
    "    # Use the 'hybrid_recommender' variable from the training cell\n",
    "    # --- TRY REDUCING n_neg_samples FIRST ---\n",
    "    print(\"Evaluating with n_neg_samples=20 for speed test...\")\n",
    "    hybrid_results = hybrid_evaluator.evaluate_model(hybrid_recommender, n_neg_samples=20) # Reduced samples\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    print(\"\\nHybrid Model Evaluation Results (n_neg_samples=20):\") # Updated print\n",
    "    print(hybrid_results)\n",
    "\n",
    "    # --- Optional: Run with full samples if the reduced one was fast enough ---\n",
    "    # print(\"\\n--- Starting Evaluation of HybridNCFRecommender (n_neg_samples=100) ---\")\n",
    "    # hybrid_results_full = hybrid_evaluator.evaluate_model(hybrid_recommender, n_neg_samples=100)\n",
    "    # print(\"\\nHybrid Model Evaluation Results (n_neg_samples=100):\")\n",
    "    # print(hybrid_results_full)\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
